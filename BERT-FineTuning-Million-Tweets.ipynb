{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"t140 = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv',\n                   sep=',',header=None,encoding='latin', verbose=True)\nt140.columns = ['label','id','timestamp','unknown','username','tweet']","execution_count":2,"outputs":[{"output_type":"stream","text":"Tokenization took: 143.02 ms\nType conversion took: 201.09 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 115.17 ms\nType conversion took: 177.71 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 114.27 ms\nType conversion took: 173.01 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 113.56 ms\nType conversion took: 176.05 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 124.74 ms\nType conversion took: 193.43 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 169.26 ms\nType conversion took: 233.25 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 170.89 ms\nType conversion took: 231.02 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 175.66 ms\nType conversion took: 205.34 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 119.94 ms\nType conversion took: 157.59 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 113.96 ms\nType conversion took: 160.79 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 116.78 ms\nType conversion took: 156.51 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 108.61 ms\nType conversion took: 157.17 ms\nParser memory cleanup took: 0.01 ms\nTokenization took: 22.87 ms\nType conversion took: 30.21 ms\nParser memory cleanup took: 0.01 ms\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"t140.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   label          id                     timestamp   unknown         username  \\\n0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n\n                                               tweet  \n0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n1  is upset that he can't update his Facebook by ...  \n2  @Kenichan I dived many times for the ball. Man...  \n3    my whole body feels itchy and like its on fire   \n4  @nationwideclass no, it's not behaving at all....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>id</th>\n      <th>timestamp</th>\n      <th>unknown</th>\n      <th>username</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"t140 = t140[['label','tweet']]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t140.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"   label                                              tweet\n0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n1      0  is upset that he can't update his Facebook by ...\n2      0  @Kenichan I dived many times for the ball. Man...\n3      0    my whole body feels itchy and like its on fire \n4      0  @nationwideclass no, it's not behaving at all....","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"t140.groupby('label').count()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"        tweet\nlabel        \n0      800000\n4      800000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>800000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>800000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"t140['label'] = t140['label'].apply(lambda x: 0 if x== 0 else 1)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t140_sub = t140.groupby('label').apply(lambda x: x.sample(20000).reset_index(drop=True))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t140_sub.reset_index(drop=True, inplace=True)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t140_sub.head()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"   label                                              tweet\n0      0        I'm up damn so early  gotta go to the gym..\n1      0  @jimanda2 Oh, no.  Poor baby - at 5 they have ...\n2      0  @caseynugget Ha, cross your fingers. It's boun...\n3      0  @The_Rooster I'm not at all keen to give this ...\n4      0  I can't fall asleeeep!   tiff needs to hurry u...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>I'm up damn so early  gotta go to the gym..</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>@jimanda2 Oh, no.  Poor baby - at 5 they have ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>@caseynugget Ha, cross your fingers. It's boun...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>@The_Rooster I'm not at all keen to give this ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I can't fall asleeeep!   tiff needs to hurry u...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"t140_sub.groupby('label').count()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"       tweet\nlabel       \n0      20000\n1      20000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nhashtags = re.compile(r\"^#\\S+|\\s#\\S+\")\nmentions = re.compile(r\"^@\\S+|\\s@\\S+\")\nurls = re.compile(r\"https?://\\S+\")\n\ndef process_text(text):\n    text = hashtags.sub(' hashtag', text)\n    text = mentions.sub(' entity', text)\n    return text.strip().lower()\n  \ndef match_expr(pattern, string):\n    return not pattern.search(string) == None\n\ndef get_data_wo_urls(dataset):\n    link_with_urls = dataset.text.apply(lambda x: match_expr(urls, x))\n    return dataset[[not e for e in link_with_urls]]","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t140_sub.tweet = t140_sub.tweet.apply(process_text)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t140_sub.sample(10)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"       label                                              tweet\n26116      1  entity nope. had a burger. and half a bag of d...\n22708      1  bye guys  i have to sleep for a bit.  my frien...\n13490      0        watching pursuit of happiness. im gonna cry\n31103      1  http://twitter.com/cruisemaniac/statuses/17528...\n31009      1  entity aww thanks girl you know your my fierce...\n29583      1  entity you all have this game, i'm cheering yo...\n36154      1  entity thank you for tweeting about entity tha...\n1416       0  who let the dogs out?  whou whou whou-whou my ...\n9022       0  weather sucks so much my blackberry is now a b...\n20213      1  well im done swiming. now im some what sleepy....","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26116</th>\n      <td>1</td>\n      <td>entity nope. had a burger. and half a bag of d...</td>\n    </tr>\n    <tr>\n      <th>22708</th>\n      <td>1</td>\n      <td>bye guys  i have to sleep for a bit.  my frien...</td>\n    </tr>\n    <tr>\n      <th>13490</th>\n      <td>0</td>\n      <td>watching pursuit of happiness. im gonna cry</td>\n    </tr>\n    <tr>\n      <th>31103</th>\n      <td>1</td>\n      <td>http://twitter.com/cruisemaniac/statuses/17528...</td>\n    </tr>\n    <tr>\n      <th>31009</th>\n      <td>1</td>\n      <td>entity aww thanks girl you know your my fierce...</td>\n    </tr>\n    <tr>\n      <th>29583</th>\n      <td>1</td>\n      <td>entity you all have this game, i'm cheering yo...</td>\n    </tr>\n    <tr>\n      <th>36154</th>\n      <td>1</td>\n      <td>entity thank you for tweeting about entity tha...</td>\n    </tr>\n    <tr>\n      <th>1416</th>\n      <td>0</td>\n      <td>who let the dogs out?  whou whou whou-whou my ...</td>\n    </tr>\n    <tr>\n      <th>9022</th>\n      <td>0</td>\n      <td>weather sucks so much my blackberry is now a b...</td>\n    </tr>\n    <tr>\n      <th>20213</th>\n      <td>1</td>\n      <td>well im done swiming. now im some what sleepy....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":15,"outputs":[{"output_type":"stream","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\n\n# Load the BERT tokenizer.\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","execution_count":16,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"},{"output_type":"stream","text":"Loading BERT tokenizer...\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a4c7ccd7df47a78886351bb95970a7"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = t140_sub.label.values\nsentences = t140_sub.tweet.values","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize all of the sentences and map the tokens to thier word IDs.\ninput_ids = []\nattention_masks = []\n\n# For every sentence...\nfor sent in tqdm(sentences):\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 128,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Print sentence 0, now as a list of IDs.\nprint('Original: ', sentences[0])\nprint('Token IDs:', input_ids[0])","execution_count":18,"outputs":[{"output_type":"stream","text":"100%|██████████| 40000/40000 [00:28<00:00, 1395.35it/s]\n","name":"stderr"},{"output_type":"stream","text":"Original:  i'm up damn so early  gotta go to the gym..\nToken IDs: tensor([  101,  1045,  1005,  1049,  2039,  4365,  2061,  2220, 10657,  2175,\n         2000,  1996,  9726,  1012,  1012,   102,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split\n\n# Combine the training inputs into a TensorDataset.\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n# Create a 90-10 train-validation split.\n\n# Calculate the number of samples to include in each set.\ntrain_size = int(0.85 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","execution_count":19,"outputs":[{"output_type":"stream","text":"34,000 training samples\n6,000 validation samples\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n# The DataLoader needs to know our batch size for training, so we specify it \n# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n# size of 16 or 32.\nbatch_size = 32\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertForSequenceClassification, AdamW, BertConfig\n\n# Load BertForSequenceClassification, the pretrained BERT model with a single \n# linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the GPU.\nmodel.cuda()","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb779f8e108447538d9857439d07c153"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38641422e30f4c3bb42a965a038ed4c5"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n# I believe the 'W' stands for 'Weight Decay fix\"\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs. The BERT authors recommend between 2 and 4. \n# We chose to run for 4, but we'll see later that this may be over-fitting the\n# training data.\nepochs = 4\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_dataloader)","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"1063"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport numpy as np\n\n# This training code is based on the `run_glue.py` script here:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass.\n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # The documentation for this `model` function is here: \n        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n        loss, logits = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n        \n    # ========================================\n    #       Validation - After Every Epoch\n    # ========================================\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using \n        # the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n            # Get the \"logits\" output by the model. The \"logits\" are the output\n            # values prior to applying an activation function like the softmax.\n            (loss, logits) = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n            \n        # Accumulate the validation loss.\n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n\n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","execution_count":27,"outputs":[{"output_type":"stream","text":"\n======== Epoch 1 / 4 ========\nTraining...\n  Batch    40  of  1,063.    Elapsed: 0:00:16.\n  Batch    80  of  1,063.    Elapsed: 0:00:31.\n  Batch   120  of  1,063.    Elapsed: 0:00:47.\n  Batch   160  of  1,063.    Elapsed: 0:01:02.\n  Batch   200  of  1,063.    Elapsed: 0:01:17.\n  Batch   240  of  1,063.    Elapsed: 0:01:32.\n  Batch   280  of  1,063.    Elapsed: 0:01:47.\n  Batch   320  of  1,063.    Elapsed: 0:02:02.\n  Batch   360  of  1,063.    Elapsed: 0:02:18.\n  Batch   400  of  1,063.    Elapsed: 0:02:33.\n  Batch   440  of  1,063.    Elapsed: 0:02:48.\n  Batch   480  of  1,063.    Elapsed: 0:03:03.\n  Batch   520  of  1,063.    Elapsed: 0:03:18.\n  Batch   560  of  1,063.    Elapsed: 0:03:34.\n  Batch   600  of  1,063.    Elapsed: 0:03:49.\n  Batch   640  of  1,063.    Elapsed: 0:04:04.\n  Batch   680  of  1,063.    Elapsed: 0:04:19.\n  Batch   720  of  1,063.    Elapsed: 0:04:34.\n  Batch   760  of  1,063.    Elapsed: 0:04:50.\n  Batch   800  of  1,063.    Elapsed: 0:05:05.\n  Batch   840  of  1,063.    Elapsed: 0:05:20.\n  Batch   880  of  1,063.    Elapsed: 0:05:35.\n  Batch   920  of  1,063.    Elapsed: 0:05:51.\n  Batch   960  of  1,063.    Elapsed: 0:06:06.\n  Batch 1,000  of  1,063.    Elapsed: 0:06:21.\n  Batch 1,040  of  1,063.    Elapsed: 0:06:36.\n\n  Average training loss: 0.42\n  Training epcoh took: 0:06:45\n\nRunning Validation...\n  Accuracy: 0.85\n  Validation Loss: 0.36\n  Validation took: 0:00:22\n\n======== Epoch 2 / 4 ========\nTraining...\n  Batch    40  of  1,063.    Elapsed: 0:00:15.\n  Batch    80  of  1,063.    Elapsed: 0:00:30.\n  Batch   120  of  1,063.    Elapsed: 0:00:46.\n  Batch   160  of  1,063.    Elapsed: 0:01:01.\n  Batch   200  of  1,063.    Elapsed: 0:01:16.\n  Batch   240  of  1,063.    Elapsed: 0:01:31.\n  Batch   280  of  1,063.    Elapsed: 0:01:47.\n  Batch   320  of  1,063.    Elapsed: 0:02:02.\n  Batch   360  of  1,063.    Elapsed: 0:02:17.\n  Batch   400  of  1,063.    Elapsed: 0:02:32.\n  Batch   440  of  1,063.    Elapsed: 0:02:47.\n  Batch   480  of  1,063.    Elapsed: 0:03:03.\n  Batch   520  of  1,063.    Elapsed: 0:03:18.\n  Batch   560  of  1,063.    Elapsed: 0:03:33.\n  Batch   600  of  1,063.    Elapsed: 0:03:48.\n  Batch   640  of  1,063.    Elapsed: 0:04:03.\n  Batch   680  of  1,063.    Elapsed: 0:04:18.\n  Batch   720  of  1,063.    Elapsed: 0:04:34.\n  Batch   760  of  1,063.    Elapsed: 0:04:49.\n  Batch   800  of  1,063.    Elapsed: 0:05:04.\n  Batch   840  of  1,063.    Elapsed: 0:05:19.\n  Batch   880  of  1,063.    Elapsed: 0:05:35.\n  Batch   920  of  1,063.    Elapsed: 0:05:50.\n  Batch   960  of  1,063.    Elapsed: 0:06:05.\n  Batch 1,000  of  1,063.    Elapsed: 0:06:20.\n  Batch 1,040  of  1,063.    Elapsed: 0:06:36.\n\n  Average training loss: 0.28\n  Training epcoh took: 0:06:44\n\nRunning Validation...\n  Accuracy: 0.85\n  Validation Loss: 0.38\n  Validation took: 0:00:22\n\n======== Epoch 3 / 4 ========\nTraining...\n  Batch    40  of  1,063.    Elapsed: 0:00:15.\n  Batch    80  of  1,063.    Elapsed: 0:00:31.\n  Batch   120  of  1,063.    Elapsed: 0:00:46.\n  Batch   160  of  1,063.    Elapsed: 0:01:01.\n  Batch   200  of  1,063.    Elapsed: 0:01:16.\n  Batch   240  of  1,063.    Elapsed: 0:01:31.\n  Batch   280  of  1,063.    Elapsed: 0:01:47.\n  Batch   320  of  1,063.    Elapsed: 0:02:02.\n  Batch   360  of  1,063.    Elapsed: 0:02:17.\n  Batch   400  of  1,063.    Elapsed: 0:02:32.\n  Batch   440  of  1,063.    Elapsed: 0:02:48.\n  Batch   480  of  1,063.    Elapsed: 0:03:03.\n  Batch   520  of  1,063.    Elapsed: 0:03:18.\n  Batch   560  of  1,063.    Elapsed: 0:03:33.\n  Batch   600  of  1,063.    Elapsed: 0:03:49.\n  Batch   640  of  1,063.    Elapsed: 0:04:04.\n  Batch   680  of  1,063.    Elapsed: 0:04:19.\n  Batch   720  of  1,063.    Elapsed: 0:04:34.\n  Batch   760  of  1,063.    Elapsed: 0:04:50.\n  Batch   800  of  1,063.    Elapsed: 0:05:05.\n  Batch   840  of  1,063.    Elapsed: 0:05:20.\n  Batch   880  of  1,063.    Elapsed: 0:05:35.\n  Batch   920  of  1,063.    Elapsed: 0:05:51.\n  Batch   960  of  1,063.    Elapsed: 0:06:06.\n  Batch 1,000  of  1,063.    Elapsed: 0:06:21.\n  Batch 1,040  of  1,063.    Elapsed: 0:06:36.\n\n  Average training loss: 0.18\n  Training epcoh took: 0:06:45\n\nRunning Validation...\n  Accuracy: 0.85\n  Validation Loss: 0.45\n  Validation took: 0:00:22\n\n======== Epoch 4 / 4 ========\nTraining...\n  Batch    40  of  1,063.    Elapsed: 0:00:15.\n  Batch    80  of  1,063.    Elapsed: 0:00:31.\n  Batch   120  of  1,063.    Elapsed: 0:00:46.\n  Batch   160  of  1,063.    Elapsed: 0:01:01.\n  Batch   200  of  1,063.    Elapsed: 0:01:16.\n  Batch   240  of  1,063.    Elapsed: 0:01:31.\n  Batch   280  of  1,063.    Elapsed: 0:01:47.\n  Batch   320  of  1,063.    Elapsed: 0:02:02.\n  Batch   360  of  1,063.    Elapsed: 0:02:17.\n  Batch   400  of  1,063.    Elapsed: 0:02:33.\n  Batch   440  of  1,063.    Elapsed: 0:02:48.\n  Batch   480  of  1,063.    Elapsed: 0:03:03.\n  Batch   520  of  1,063.    Elapsed: 0:03:18.\n  Batch   560  of  1,063.    Elapsed: 0:03:34.\n  Batch   600  of  1,063.    Elapsed: 0:03:49.\n  Batch   640  of  1,063.    Elapsed: 0:04:04.\n  Batch   680  of  1,063.    Elapsed: 0:04:19.\n  Batch   720  of  1,063.    Elapsed: 0:04:35.\n  Batch   760  of  1,063.    Elapsed: 0:04:50.\n  Batch   800  of  1,063.    Elapsed: 0:05:05.\n  Batch   840  of  1,063.    Elapsed: 0:05:21.\n  Batch   880  of  1,063.    Elapsed: 0:05:36.\n  Batch   920  of  1,063.    Elapsed: 0:05:51.\n  Batch   960  of  1,063.    Elapsed: 0:06:06.\n  Batch 1,000  of  1,063.    Elapsed: 0:06:22.\n  Batch 1,040  of  1,063.    Elapsed: 0:06:37.\n\n  Average training loss: 0.12\n  Training epcoh took: 0:06:46\n\nRunning Validation...\n  Accuracy: 0.84\n  Validation Loss: 0.56\n  Validation took: 0:00:22\n\nTraining complete!\nTotal training took 0:28:28 (h:mm:ss)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\n# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n\noutput_dir = 'bert_model_save/'\n\n# Create output directory if needed\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(\"Saving model to %s\" % output_dir)\n\n# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n# They can then be reloaded using `from_pretrained()`\nmodel_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\nmodel_to_save.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n# Good practice: save your training arguments together with the trained model\n# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n","execution_count":33,"outputs":[{"output_type":"stream","text":"Saving model to bert_model_save/\n","name":"stdout"},{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"('bert_model_save/vocab.txt',\n 'bert_model_save/special_tokens_map.json',\n 'bert_model_save/added_tokens.json')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# Display floats with two decimal places.\npd.set_option('precision', 2)\n\n# Create a DataFrame from our training statistics.\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index.\ndf_stats = df_stats.set_index('epoch')\n\n# A hack to force the column headers to wrap.\n#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n\n# Display the table.\ndf_stats","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\nepoch                                                                         \n1               0.42         0.36           0.85       0:06:45         0:00:22\n2               0.28         0.38           0.85       0:06:44         0:00:22\n3               0.18         0.45           0.85       0:06:45         0:00:22\n4               0.12         0.56           0.84       0:06:46         0:00:22","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n      <th>Valid. Accur.</th>\n      <th>Training Time</th>\n      <th>Validation Time</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.42</td>\n      <td>0.36</td>\n      <td>0.85</td>\n      <td>0:06:45</td>\n      <td>0:00:22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.28</td>\n      <td>0.38</td>\n      <td>0.85</td>\n      <td>0:06:44</td>\n      <td>0:00:22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.18</td>\n      <td>0.45</td>\n      <td>0.85</td>\n      <td>0:06:45</td>\n      <td>0:00:22</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.12</td>\n      <td>0.56</td>\n      <td>0.84</td>\n      <td>0:06:46</td>\n      <td>0:00:22</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n# Plot the learning curve.\nplt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\nplt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\n# Label the plot.\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks([1, 2, 3, 4])\n\nplt.show()","execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 864x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction on test set\n\nprint('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npredictions , true_labels = [], []\n\n# Predict \nfor batch in validation_dataloader:\n  # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and \n  # speeding up prediction\n    with torch.no_grad():\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n    logits = outputs[0]\n\n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    # Store predictions and true labels\n    predictions.append(logits)\n    true_labels.append(label_ids)\n\nprint('DONE.')","execution_count":37,"outputs":[{"output_type":"stream","text":"Predicting labels for 40,000 test sentences...\nDONE.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_labels_list = []\npredictions_list = []\nfor labels in true_labels:\n    true_labels_list.extend(labels.tolist())\n\nfor prediction in predictions:\n    predictions_list.extend(np.argmax(prediction,axis=1).flatten())\n    \nlen(true_labels_list),len(predictions_list)","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"(6000, 6000)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(true_labels_list,predictions_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_names = ['Class 0','Class 1']\nprint(classification_report(true_labels_list,predictions_list,target_names=target_names))","execution_count":64,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n     Class 0       0.84      0.85      0.84      3016\n     Class 1       0.84      0.84      0.84      2984\n\n    accuracy                           0.84      6000\n   macro avg       0.84      0.84      0.84      6000\nweighted avg       0.84      0.84      0.84      6000\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"6000"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"6000"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}